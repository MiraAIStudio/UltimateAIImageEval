The prompts used in the ImagenHub benchmark are derived from a combination of DrawBench, DiffusionDB, ABC-6K, and ChromAIca with Midjourney Prompt Generator. Although the prompts have been selected and categorized, the evaluation value of some prompts is limited. A detailed analysis is provided below:

**Limitations of Prompt Sources:**

*   **DiffusionDB, ABC-6K, ChromAIca + Midjourney Prompt Generator:** The prompt categories from these sources are relatively homogeneous, lacking diversity and generalizability. This makes it difficult to comprehensively evaluate the performance of models across different domains and tasks.

*   **DrawBench:** The design of some categories in DrawBench is not rigorous enough, which affects the scientific validity of the evaluation.

    *   **Conflicting Category:** This category aims to test the model's ability to handle conceptual conflicts. However, the existing prompts mainly focus on conflicts between unrelated objects, lacking the examination of more subtle and complex conflict scenarios. For example, more challenging conflicts should involve objects or concepts that share a context but have contradictory functions, purposes, or usual interactions.

    *   **Descriptions Category:** This category examines the model's understanding ability by indirectly describing objects rather than directly naming them. However, this indirect description may introduce ambiguity, leading to multiple valid interpretations. For example, the prompt uid33 ("A device consisting of a circular canopy of cloth on a folding metal frame supported by a central rod, used as protection against rain or sometimes sun.") may generate an "umbrella" or an "umbrella-like object," making it difficult to objectively evaluate the quality of the model's generation. Therefore, the evaluation of this category should consider the possibility of multiple reasonable interpretations and regard it as a test of the model's *interpretive ability* rather than just its *generative ability*.

    *   **Misspellings and Rare Words Categories:** These two categories test the model's robustness to misspellings and rare vocabulary, respectively. However, under the current prompts, almost all models perform poorly, failing to effectively differentiate model performance. The validity of these two categories depends on whether any model can show a certain degree of robustness to errors. In practice, they provide no evaluation value.

    *   **Reddit Category:** The testing objective of this category is unclear, and there is a lack of clear basis for selecting prompts, making it difficult to evaluate its contribution to the assessment of model performance.

**Transparency of Dataset Selection:**

*   **DiffusionDB:** DiffusionDB contains 14 million prompts and images generated by real users, making it the first large-scale text-to-image prompt dataset. However, the process by which ImagenHub selected 40 prompts from it lacks transparency, and the selection criteria are not specified. Random selection is not the optimal solution, as it may lead to insufficient representativeness of the prompts.

* **ABC-6K:** The ABC-6K dataset includes prompts with at least two color words, which overlaps with the Colors and Conflicting categories of DrawBench.